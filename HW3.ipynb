{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Neural Networks\n",
    "\n",
    "In the previous exercise you implemented a binary classifier with one linear layer on a small portion of CIFAR-10. In this exercise, you will first implement a multi-class logistic regression model followed by a three layer neural network.\n",
    "\n",
    "## Submission guidelines:\n",
    "\n",
    "Your zip should include the following files only:\n",
    "```\n",
    "- HW3.ipynb\n",
    "- functions/\n",
    "    - classifier.py\n",
    "    - layers.py\n",
    "    - losses.py\n",
    "    - neural_net.py\n",
    "```\n",
    "Name the file `ex3_ID.zip` if you submit alone of `ex3_ID1_ID2.zip` if you submit in pairs. Do **not** include the data. \n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This jupyter notebook contains all the step by step instructions needed for this exercise.\n",
    "1. Write **efficient vectorized** code whenever instructed. \n",
    "1. You should add as many tests as you see fit. Tests will not be graded nor checked.\n",
    "1. Do not change the functions we provided you. \n",
    "1. Write your functions in the instructed python modules only. All the logic you write is imported and used using this jupyter notebook. You are allowed to add functions as long as they are located in the python modules and are imported properly.\n",
    "1. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/) and [numpy](https://www.numpy.org/devdocs/reference/) only.\n",
    "1. Your code must run without errors. Use `python 3` and at least `numpy 1.15.4`. Before submitting the exercise, restart the kernel and run the notebook from start to finish to make sure everything works. \n",
    "1. Answers to qualitative questions should be written in **markdown** cells (with $\\LaTeX$ support)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "print(\"Python version: \", platform.python_version())\n",
    "print(\"Numpy version: \", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## Load Data - CIFAR-10\n",
    "\n",
    "The next few cells will download and extract CIFAR-10 into `datasets/cifar10/` - notice you can copy and paste this dataset from the previous exercise or just download it again. The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. The dataset is divided into five training batches and one test batch, each with 10,000 images. The test batch contains exactly 1,000 randomly-selected images from each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_cifar10\n",
    "URL = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "PATH = 'datasets/cifar10/' # the script will create required directories\n",
    "load_cifar10.maybe_download_and_extract(URL, PATH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR10_PATH = os.path.join(PATH, 'cifar-10-batches-py')\n",
    "X_train, y_train, X_test, y_test = load_cifar10.load(CIFAR10_PATH) # load the entire data\n",
    "\n",
    "# define a splitting for the data\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_testing = 1000\n",
    "\n",
    "# add a validation dataset for hyperparameter optimization\n",
    "mask = range(num_training)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "mask = range(num_validation)\n",
    "X_val = X_test[mask]\n",
    "y_val = y_test[mask]\n",
    "mask = range(num_validation, num_validation+num_testing)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "# float64\n",
    "X_train = X_train.astype(np.float64)\n",
    "X_val = X_val.astype(np.float64)\n",
    "X_test = X_test.astype(np.float64)\n",
    "\n",
    "# subtract the mean from all the images in the batch\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "\n",
    "# flatten all the images in the batch (make sure you understand why this is needed)\n",
    "X_train = np.reshape(X_train, newshape=(X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, newshape=(X_val.shape[0], -1)) \n",
    "X_test = np.reshape(X_test, newshape=(X_test.shape[0], -1)) \n",
    "\n",
    "# add a bias term to all images in the batch\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))]) \n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))]) \n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))]) \n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X, y, n):\n",
    "    rand_items = np.random.randint(0, X.shape[0], size=n)\n",
    "    images = X[rand_items]\n",
    "    labels = y[rand_items]\n",
    "    return X, y\n",
    "\n",
    "def make_random_grid(x, y, n=4):\n",
    "    rand_items = np.random.randint(0, x.shape[0], size=n)\n",
    "    images = x[rand_items]\n",
    "    labels = y[rand_items]\n",
    "    grid = np.hstack((np.asarray((vec_2_img(i) + mean_image), dtype=np.int) for i in images))\n",
    "    print(' '.join('%13s' % classes[labels[j]] for j in range(4)))\n",
    "    return grid\n",
    "\n",
    "def vec_2_img(x):\n",
    "    x = np.reshape(x[:-1], (32, 32, 3))\n",
    "    return x\n",
    "\n",
    "X_batch, y_batch = get_batch(X_test, y_test, 4)\n",
    "plt.imshow(make_random_grid(X_batch, y_batch));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the file `functions/classifier.py`. The constructor of the `LogisticRegression` class takes as input the dataset and labels in order to create appropriate parameters. Notice we are using the bias trick and only use the matrix `w` for convenience. Since we already have a (random) model, we can start predicting classes on images. Complete the method `predict` in the `LogisticRegression` class. **5 points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.classifier import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "X_batch, y_batch = get_batch(X_train, y_train, 4)\n",
    "plt.imshow(make_random_grid(X_batch, y_batch));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join('%13s' % classes[y_pred[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model accuracy: \", classifier.calc_accuracy(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy\n",
    "\n",
    "Open the file `functions/losses.py`. Complete the function `softmax_loss_vectorized` using vectorized code. This function takes as input the weights `W`, data `X`, labels `y` and a regularization term `reg` and outputs the calculated loss as a single number and the gradients with respect to W. Don't forget the regularization. **5 points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.losses import softmax_loss_vectorized\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss_naive, grad_naive = softmax_loss_vectorized(W, X_val, y_val, 0.00000)\n",
    "print ('loss: %f' % (loss_naive, ))\n",
    "print ('sanity check: %f' % (-np.log(0.1))) # should be close but not the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? **Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following cell to test your implementation of the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.losses import grad_check\n",
    "\n",
    "loss, grad = softmax_loss_vectorized(W, X_val, y_val, 1)\n",
    "f = lambda w: softmax_loss_vectorized(W, X_val, y_val, 1)[0]\n",
    "grad_numerical = grad_check(f, W, grad, num_checks=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.classifier import LogisticRegression\n",
    "\n",
    "logistic = LogisticRegression(X_train, y_train)\n",
    "loss_history = logistic.train(X_train, y_train, \n",
    "                         learning_rate=1e-7,\n",
    "                         reg=5e4, \n",
    "                         num_iters=1500,\n",
    "                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training accuracy: \", logistic.calc_accuracy(X_train, y_train))\n",
    "print(\"Testing accuracy: \", logistic.calc_accuracy(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the validation set to tune hyperparameters by training different models (using the training dataset) and evaluating the performance using the validation dataset. Save the results in a dictionary mapping tuples of the form `(learning_rate, batch_size)` to tuples of the form `(training_accuracy, validation_accuracy)`. Finally, you should evaluate the best model on the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are encouraged to experiment with additional values\n",
    "learning_rates = [1e-7, 5e-6]\n",
    "regularization_strengths = [5e4, 1e5, 5e3, 1e2]\n",
    "results = {}\n",
    "best_val = -1   # The highest validation accuracy that we have seen so far.\n",
    "best_logistic = None # The LogisticRegression object that achieved the highest validation score.\n",
    "################################################################################\n",
    "#                            START OF YOUR CODE                                #\n",
    "################################################################################\n",
    "pass\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print ('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print ('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
    "\n",
    "test_accuracy = logistic.calc_accuracy(X_test, y_test)\n",
    "print ('Binary logistic regression on raw pixels final test set accuracy: %f' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "w = best_logistic.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "The implementation of linear regression was (hopefully) simple yet not very modular since the layer, loss and gradient were calculated as a single monolithic function. This would become impractical as we move towards bigger models. As a warmup towards `PyTorch`, we want to build networks using a more modular design so that we can implement different layer types in isolation and easily integrate them together into models with different architectures.\n",
    "\n",
    "This logic of isolation & integration is at the heart of all popular deep learning frameworks, and is based on two methods each layer holds - a forward and backward pass. The forward function will receive inputs, weights and other parameters and will return both an output and a cache object storing data needed for the backward pass. The backward pass will receive upstream derivatives and the cache, and will return gradients with respect to the inputs and weights. By implementing several types of layers this way, we will be able to easily combine them to build classifiers with different architectures with relative ease.\n",
    "\n",
    "We will implement a neural network to obtain better results on CIFAR-10. If you were careful, you should have got a classification accuracy of over 38% on the test set using a simple single layer network. However, using multiple layers we could reach around 50% accuracy. Our neural network will be implemented in the file `functions/neural_net.py`. We will train this network using softmax loss and L2 regularization and a ReLU non-linearity after the first two fully connected layers.\n",
    "\n",
    "### Fully Connected Layer: Forward Pass.\n",
    "\n",
    "Open the file `functions/layers.py` and implement the function `fc_forward` **7.5 points**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "from functions.layers import * \n",
    "\n",
    "num_instances = 5\n",
    "input_shape = (11, 7, 3)\n",
    "output_shape = 4\n",
    "\n",
    "X = np.random.randn(num_instances * np.prod(input_shape)).reshape(num_instances, *input_shape)\n",
    "W = np.random.randn(np.prod(input_shape) * output_shape).reshape(np.prod(input_shape), output_shape)\n",
    "b = np.random.randn(output_shape)\n",
    "\n",
    "out, _ = fc_forward(X, W, b)\n",
    "\n",
    "correct_out = np.array([[16.77132953,  1.43667172, -15.60205534,   7.15789287],\n",
    "                        [ -8.5994206,  7.59104298,  10.92160126,  17.19394331],\n",
    "                        [ 4.77874003,  2.25606192,  -6.10944859,  14.76954561],\n",
    "                        [21.21222953, 17.82329258,   4.53431782,  -9.88327913],\n",
    "                        [18.83041801, -2.55273817,  14.08484003,  -3.99196171]])\n",
    "\n",
    "print(np.isclose(out, correct_out, rtol=1e-8).all()) # simple test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Layer: Backward Pass\n",
    "\n",
    "Open the file `functions/layers.py` and implement the function `fc_backward` **7.5 points**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: fc_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: fc_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: fc_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "out, cache = fc_forward(x,w,b)\n",
    "dx, dw, db = fc_backward(dout, cache)\n",
    "\n",
    "np.isclose(dw, dw_num, rtol=1e-8).all() # simple test\n",
    "np.isclose(dx, dx_num, rtol=1e-8).all() # simple test\n",
    "np.isclose(db, db_num, rtol=1e-8).all() # simple test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU: Forward Pass\n",
    "\n",
    "Open the file `functions/layers.py` and implement the function `relu_forward` **7.5 points**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "print(np.isclose(out, correct_out, rtol=1e-8).all()) # simple test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU: Backward Pass\n",
    "\n",
    "Open the file `functions/layers.py` and implement the function `relu_backward` **7.5 points**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "xx, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "np.isclose(dx, dx_num, rtol=1e-8).all()  # simple test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional**: you are given two helper functions in `functions/layers.py` - `fc_relu_forward` and `fc_relu_backward`. You might find it beneficial to use dedicated functions to calculate the forward and backward outputs of a fully connected layer immediately followed by a ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Network\n",
    "\n",
    "First, notice that we are leaving behind the bias trick and removing the bias from each image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([x[:-1] for x in X_train])\n",
    "X_val = np.array([x[:-1] for x in X_val])\n",
    "X_test = np.array([x[:-1] for x in X_test])\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the file `functions/neural_net.py` and complete the class `ThreeLayerNet`. All the implementation details are available in the file itself. Read the documentation carefully since the class of this network is slightly different from the network in the previous section of this exercise. **50 points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.neural_net import ThreeLayerNet\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "model = ThreeLayerNet(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = model.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=1500, batch_size=200,\n",
    "            learning_rate=1e-3, reg=0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc = (model.predict(X_val) == y_val).mean()\n",
    "print ('Validation accuracy: ', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['train_acc_history'], label='train')\n",
    "plt.plot(stats['val_acc_history'], label='val')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the validation set to tune hyperparameters by training different models (using the training dataset) and evaluating the performance using the validation dataset. Save the results in a dictionary mapping tuples of the form `(learning_rate, hidden_size, regularization)` to tuples of the form `(training_accuracy, validation_accuracy)`. You should evaluate the best model on the testing dataset and print out the training, validation and testing accuracies for each of the models and provide a clear visualization. Highlight the best model w.r.t the testing accuracy. **10 points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are encouraged to experiment with additional values\n",
    "learning_rates = [1e-3, 2e-3, 4e-3] # this time its up to you\n",
    "hidden_sizes = [20, 40, 60, 80] # this time its up to you\n",
    "regularizations = [1e-3, 2e-3] # this time its up to you\n",
    "\n",
    "results = {}\n",
    "best_val = -1   \n",
    "best_net = None \n",
    "################################################################################\n",
    "#                            START OF YOUR CODE                                #\n",
    "################################################################################\n",
    "input_size = 32 * 32 * 3\n",
    "num_classes = 10\n",
    "for lr in learning_rates:\n",
    "    for hs in hidden_sizes:\n",
    "        for reg in regularizations:\n",
    "            model = ThreeLayerNet(input_size, hs, num_classes)\n",
    "            stats = model.train(X_train, y_train, X_val, y_val,\n",
    "                num_iters=2000, batch_size=200,\n",
    "                learning_rate=lr, reg=reg, verbose=False)\n",
    "    \n",
    "            val_acc = (model.predict(X_val) == y_val).mean()\n",
    "            train_accuracy =  (model.predict(X_train) == y_train).mean()\n",
    "            val_accuracy =  (model.predict(X_val) == y_val).mean()\n",
    "            results[(lr, hs, reg)] = (train_accuracy, val_accuracy)\n",
    "            print ('lr %e, hidden size %d, reg %e - train accuracy: %f val accuracy: %f' % ( lr, hs, reg, train_accuracy, val_accuracy))\n",
    "\n",
    "            if val_accuracy > best_val:\n",
    "                best_val = val_accuracy\n",
    "                best_net = model\n",
    "                \n",
    "print ('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
    "\n",
    "test_accuracy = (best_net.predict(X_test) == y_test).mean()\n",
    "print ('ThreeLayerNet final test set accuracy: %f' % test_accuracy)\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 2:\n",
    "What can you say about the training? Why does it take much longer to train? How could you speed up computation? What would happen to the network accuracy and training time when adding additional layer? What about additional hidden neurons?\n",
    "\n",
    "**Your answer:** \n",
    "The trainig process takes longer than, for example, a one layer perceptron, since we have much more additions and\n",
    "multiplications in each iteration.\n",
    "The best way to speed up computation is to use a suitable hardware, like a GPU and a high memory machine. The GPU \n",
    "paralelyze the computation and make the network train much faster and without compromising on accuracy.\n",
    "Addinig more layers or increasing the hidden layers size would increase the training time. As for the accuracy, \n",
    "it may improve with the increase in number of neurons and layers but up to a certain bound, of course.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus \n",
    "Train a 5 hidden layer network with varying hidden layer size and plot the loss function and train / validation accuracies. **5 points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
